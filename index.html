<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ImageInThat: Manipulating Images to Convey User Instructions to Robots</title>

    <meta name="description" content="ImageInThat: Manipulating Images to Convey User Instructions to Robots">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->

    <link href="//cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <!--script src="js/app.js"></script-->

    <style>
        .nav-pills {
            position: relative;
            display: inline;
        }

        .imtip {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>

    <link rel="stylesheet" href="./css/bulma.min.css">
    <link rel="stylesheet" href="./css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./css/bulma-slider.min.css">
    <link rel="stylesheet" href="./css/index.css">
    <link rel="stylesheet" href="./css/fontawesome.all.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/index.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5JBS73F70V"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-5JBS73F70V');
</script>

<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <div class="col-md-12 text-center" style="max-width: 1280px; margin: 0 auto; color: #333;">
                <h2>
                    <b>
                        <font size="+4">ImageInThat: Manipulating Images to <br> Convey User Instructions to Robots
                        </font>
                    </b>
                    <!-- <span style="color: orange;">Best Paper (Technical Track)</span> -->
                </h2>
            </div>

            <div class="col-md-12 text-center" style="max-width: 1280px; margin: 0 auto; color: #333;">
                <small style="font-size: 20px; margin-top: 100px; font-weight: bold;">
                    <br>
                    HRI 2025
                </small>
            </div>
        </div>
    </div>



    <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline">
                <br>
                <li><a href="https://karthikmahadevan.ca">Karthik Mahadevan<sup>1</sup></a></li>
                <li><a href="https://blainelewis.ca/">Blaine Lewis<sup>1</sup></a></li>
                <li><a href="https://jchrisli.github.io/">Jiannan Li<sup>2</sup></a></li>
                <br>
                <li><a href="https://bmutlu.github.io/">Bilge Mutlu<sup>3</sup></a></li>
                <li><a href="https://hcitang.github.io/">Anthony Tang<sup>2</sup></a></li>
                <li><a href="https://www.tovigrossman.com/">Tovi Grossman<sup>1</sup></a></li>
                <br>
                <br>
            </ul>
            <p><sup>1</sup> University of Toronto, <sup>2</sup> Singapore Management University, <sup>3</sup> University
                of Wisconsin-Madison</p>
        </div>
    </div>

    <div class="row justify-content-md-center" style="margin-top: 50px; margin-bottom: 50px;">
        <div class="col-md-2 text-center d-flex flex-column align-items-center">
            <a href="https://arxiv.org/pdf/2503.15500">
                <div style="width: 60px; height: 60px;">
                    <img src="img/paper_small.png" style="object-fit: contain; width: 100%; height: 100%;">
                </div>
            </a>
            <h4><strong>Paper</strong></h4>
        </div>

        <div class="col-md-2 text-center d-flex flex-column align-items-center">
            <a href="https://drive.google.com/file/d/1BewT7m4z2Hv3LWnUc_liNXd9UfCkYtGS/view?usp=sharing">
                <div style="width: 60px; height: 60px;">
                    <img src="img/paper_appendices.png" style="object-fit: contain; width: 100%; height: 100%;">
                </div>
            </a>
            <h4><strong>Appendices</strong></h4>
        </div>

        <div class="col-md-2 text-center d-flex flex-column align-items-center">
            <a href="https://github.com/karthikm-0/imageinthat">
                <div style="width: 60px; height: 60px;">
                    <img src="img/github.png" style="object-fit: contain; width: 100%; height: 100%;">
                </div>
            </a>
            <h4><strong>Code (will be added soon!)</strong></h4>
        </div>
    </div>

    <!-- <img src="img/pipeline.gif"> -->


    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <video width="100%" height="auto" controls>
                        <source src="img/image_in_that_video.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </div>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>

                    <div class="columns is-centered">
                        <div class="column is-four-fifths">
                            <figure style="text-align:center;">
                                <img src="img/teaser.png" width="100%" height="100%">
                            </figure>
                        </div>
                    </div>
                    <div class="content has-text-justified">
                        <p class="text-justify">
                            Foundation models are rapidly improving the capability of robots in performing everyday
                            tasks autonomously such as meal preparation, yet robots will still need to be instructed
                            by humans due to model performance, the difficulty of capturing user preferences, and
                            the need for user agency. Robots can be instructed using various methods&mdash;natural
                            language conveys immediate instructions but can be abstract or ambiguous, whereas
                            end-user programming supports longer-horizon tasks but interfaces face difficulties in
                            capturing user intent. In this work, we propose using direct manipulation of images as
                            an alternative paradigm to instruct robots, and introduce a specific instantiation
                            called ImageInThat which allows users to perform direct manipulation on images in a
                            timeline-style interface to generate robot instructions. Through a user study, we
                            demonstrate the efficacy of ImageInThat to instruct robots in kitchen manipulation
                            tasks, comparing it to a text-based natural language instruction method. The results
                            show that participants were faster with ImageInThat and preferred to use it over the
                            text-based method.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Interface</h2>
                    <div class="content has-text-justified">
                        <p class="text-justify">
                            In ImageInThat, the robot's environment is processed to create an internal
                            representation consisting of knowledge about objects and
                            fixtures. Objects are items that can be manipulated from one
                            location to another (e.g., bowls that need washing) whereas
                            fixtures are items that are immovable. Both objects and fixtures
                            can be in more than one state; for example, a cabinet is a
                            fixture that can be open or closed.

                            ImageInThat includes an editor that utilizes this internal
                            representation. The editor consists of a timeline-style interface
                            where users can manipulate images to generate robot instructions.
                            For instance, users can drag an object from one location to another
                            to generate a pick-and-place instruction. Users can also manipulate
                            fixture states, such as by clicking a cabinet to open or close it.
                            Along the way, each change appears as a step in the timeline.
                            ImageInThat also includes features to visualize changes between steps
                            inside the timeline. This includes visual filters on each step to
                            highlight changes, and a comparison view that shows the differences
                            between two steps.

                            ImageInThat also includes experimental features to allows users to
                            blend language with image to create instructions rather than relying
                            solely on image manipulation. ImageInThat can also provide some assistance
                            to the user in specifying the location of a selected object through
                            manipulation-level auto-completion. Or, the user can click a button to
                            request an image representing possible future states of the environment.
                        </p>
                    </div>
                </div>
            </div>
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <figure style="text-align:center;">
                        <img src="img/approach.png" width="100%" height="100%">
                        <figcaption style="text-align:justify; font-weight:bold; color:#333; margin-top:10px;">
                            ImageInThatâ€™s user interface, consisting of an editor (top) and a
                            timeline (bottom). The editor (A) allows users to manipulate objects and
                            fixtures in the environment, while the timeline displays the current state of the
                            environment and the desired changes. The timeline (B) shows all instructions
                            provided to the robot. Selecting a step populates it in the editor. Changes
                            between steps are made visible by contrasting changed objects and fixtures
                            from other items (C). ImageInThat automatically captions all manipulations
                            and allows them to be edited (D). The user can instruct the robot with text to
                            generate new steps automatically (E). ImageInThat tries to predict user goals
                            such as by proposing locations where objects can be placed (F) or proposing
                            future steps (G).
                        </figcaption>
                    </figure>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">User Evaluation</h2>
                    <div class="content has-text-justified">
                        <p class="text-justify">
                            We evaluated ImageInThat in a user study where participants were asked to instruct a
                            robot in simulated kitchen manipulation tasks. Participants were asked to perform
                            the tasks using both ImageInThat and a text-based natural language instruction
                            baseline.
                        </p>
                    </div>
                </div>
            </div>
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <figure style="text-align:center;">
                        <img src="img/tasks.png" width="100%" height="100%">
                        <figcaption style="text-align:justify; font-weight:bold; color:#333; margin-top:10px;">
                            Tasks performed by participants in the evaluation (left to right):
                            organizing pantry, sorting fruits, cooking stir-fry, and washing dishes.
                            Depicted here is the environment state at the beginning of each task.
                        </figcaption>
                    </figure>
                </div>
            </div>

            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <div class="content has-text-justified">
                        <p class="text-justify">
                            We found that participants were faster with ImageInThat than with the text-based
                            method across all tasks. However, we noticed that participants made different
                            types of errors with ImageInThat compared to the text-based method. For instance,
                            participants added extraneous steps in ImageInThat, which could be due to the
                            low-cost nature of adding steps in ImageInThat. In contrast, participants missed
                            more steps in the text-based method, which could be due to the difficulty in
                            keeping track of the steps in the text-based method.
                        </p>
                    </div>
                </div>
            </div>
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <figure style="text-align:center;">
                        <img src="img/results.png" width="100%" height="100%">
                        <figcaption style="text-align:justify; font-weight:bold; color:#333; margin-top:10px;">
                            Left: a plot showing the number of errors for ImageInThat and the
                            text-based method. Errors are grouped into three categories: extraneous steps,
                            missing steps, and inefficient steps, and a bar displays the total count of all
                            errors. Middle: shows the task completion time per task and all tasks together.
                            All error bars are bootstrapped 95% confidence intervals. Right: shows counts
                            of responses for the NASA-TLX questionnaire.
                        </figcaption>
                    </figure>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Translating Images to Robot Actions</h2>
                    <div class="content has-text-justified">
                        <p class="text-justify">
                            We experimented with image-to-code translation as a method to convert the
                            manipulated images into robot instructions. We prompted an LLM with
                            the skill primitives: pick, place, grasp, ungrasp, turn on faucet, turn off
                            faucet, and stack object, and the manipulated images. The LLM then generated
                            code that could be executed by the robot to perform the task.

                            We evaluated four translation tasks. The first task required the robot
                            to put a red apple into a white bowl in a table featuring a bowl, two
                            green apples and a red apple. Here, the translation was successful
                            10 out of 10 times. The second task required the robot to put both
                            green applies into the white bowl and the red apple into the bowl
                            with the pattern. This translation was also successful 10 out of 10 times.

                            The third task required the robot to put a bowl in the sink, place an orange
                            inside it, and wash the fruit by turning on the faucet. This task was successful 8 out of
                            10 times, but in 6 runs the translation incorrectly moved the second orange into the second
                            bowl as well though without moving it to the sink or washing it.

                            In the fourth task, we used part of the fourth evaluation task whereby
                            the robot needed to stack the orange donut on the plate containing the pink donut.
                            The translation succeeded 7 out of 10 times, but in 3 runs, it mistakenly included
                            code to rearrange spoons.

                        </p>
                    </div>
                </div>
            </div>
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <figure style="text-align:center;">
                        <img src="img/translation_sample.gif" width="100%" height="100%">
                        <figcaption style="text-align:justify; font-weight:bold; color:#333; margin-top:10px;">
                            We attempted to translate the images generated using ImageInThat to
                            execute the underlying instructions. One such method for translation is from
                            image to code that utilizes skill primitives by prompting an LLM. Illustrated
                            here is one execution of the instructions.
                        </figcaption>
                    </figure>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-12">
                    <h2 class="title is-3">Demo</h2>
                    <div class="content has-text-justified">
                        <p class="text-justify">
                            Below is an interactive demo of <strong>ImageInThat</strong>. You can try instructing a
                            robot by directly manipulating visual elements of the scene. This demo uses preprocessed
                            data and runs fully in the browser. However, without the server side, many features
                            are not available, including captioning of steps and autocomplete.
                        </p>
                    </div>
                </div>
            </div>

            <div class="columns is-centered mt-5">
                <div class="column is-full">
                    <div class="box" style="padding: 0;">
                        <iframe src="https://karthikm-0.github.io/imageinthat/" width="100%" height="1500"
                            style="border: none; border-radius: 8px;" title="ImageInThat Demo" loading="lazy">
                        </iframe>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <!-- <p style="text-align:center;">
        <img src="img/cover.png" width="60%" height="60%">
    </p> -->

    <!--    Approach    -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Approach</h2>
                    <div class="content has-text-justified">
                        <p class="text-justify">
                            We aim to tackle the problem of expressive behavior generation that is both adaptive to
                            user feedback and composable so that more complex behaviors can build on simpler
                            behaviors. Formally, we define being expressive as the distance between some
                            expert expressive trajectory that could be generated by an animator (or demonstrated) \(
                            \tau_{\text{expert}} \) and a robot trajectory \( \tau \). \( \text{dist}(\tau,
                            \tau_{\text{expert}}) \) can be any desirable distance metric between the two
                            trajectories, e.g., dynamic time warping (DTW). GenEM aims to minimize this distance \(
                            d^* = \min \text{dist}(\tau,\tau_{\text{expert}}) \).
                        </p>
                        <p class="text-justify">
                            Our approach uses several LLMs in a modular fashion so that
                            each LLM agent plays a distinct role. GenEM takes user language instructions \( l_{in}
                            \in L \) as input and outputs a
                            robot policy \( \pi_\theta \), which is in the form of a parameterized code.

                            Human iterative feedback \( f_i \in L \) can be used to update the policy \( \pi_\theta
                            \).
                            The policy parameters get updated one step at a time given the feedback \( f_i \), where
                            \( i
                            \in \{1,\dots, K\} \).
                            The policy can be instantiated from some initial state \( s_0 \in S \) to produce
                            trajectories \( \tau = \{s_0, a_0, \dots, a_{N-1},s_N\} \) or instantiations of
                            expressive
                            robot behavior.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </section> -->

    <!-- <p style="text-align:center;">
        <image src="img/approach.png" width="100%" height="100%">
    </p>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Studies</h2>
                    <div class="content has-text-justified">
                        <p class="text-justify">
                            We conducted two user studies to assess whether our approach, GenEM, can be used to
                            generate expressive behaviors that are perceivable by people. We generated two versions
                            of behaviors: GenEM, and GenEM with iterative Feedback (or GenEM++). In
                            both studies, all comparisons were made against behaviors designed by a professional
                            animator and implemented by a software developer, which we term the oracle animator. In
                            the first study, our goal was to assess whether behaviors that are generated
                            using GenEM and GenEM++ would be perceived similarly to the behaviors created using the
                            oracle animator. In the second study, we attempted to generate behaviors using GenEM and
                            GenEM++ that were similar to the behaviors created using the oracle animator. Both
                            studies aim
                            to demonstrate that our approach is adaptable to human feedback.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section> -->
    <!-- End Carousel -->

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
    <script>hljs.highlightAll();</script>
</body>

</html>