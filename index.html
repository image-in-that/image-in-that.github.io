<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ImageInThat: Manipulating Images to Convey User Instructions to Robots</title>

    <meta name="description" content="ImageInThat: Manipulating Images to Convey User Instructions to Robots">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->

    <link href="//cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <!--script src="js/app.js"></script-->

    <style>
        .nav-pills {
            position: relative;
            display: inline;
        }

        .imtip {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>

    <link rel="stylesheet" href="./css/bulma.min.css">
    <link rel="stylesheet" href="./css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./css/bulma-slider.min.css">
    <link rel="stylesheet" href="./css/index.css">
    <link rel="stylesheet" href="./css/fontawesome.all.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/index.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5JBS73F70V"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-5JBS73F70V');
</script>

<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <b>
                    <font size="+6">ImageInThat: Manipulating Images to Convey User Instructions to Robots</br></font>
                </b>
                <small style="font-size: 20px; margin-top: 50px;">
                    HRI 2025
                </small>
                <!-- <span style="color: orange;">Best Paper (Technical Track)</span> -->
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <br>
                    <li><a href="https://karthikmahadevan.ca">Karthik Mahadevan</a></li>
                    <li><a href="https://blainelewis.ca/">Blaine Lewis</a></li>
                    <li><a href="https://jchrisli.github.io/">Jiannan Li</a></li>
                    <br>
                    <li><a href="https://bmutlu.github.io/">Bilge Mutlu</a></li>
                    <li><a href="https://hcitang.github.io/">Anthony Tang</a></li>
                    <li><a href="https://www.tovigrossman.com/">Tovi Grossman</a></li>
                    <br>
                    <br>
                </ul>
            </div>
        </div>

        <div class="row justify-content-md-center" style="margin-top: 50px; margin-bottom: 50px;">
            <div class="col-md-2 text-center d-flex flex-column align-items-center">
                <a href="https://karthikmahadevan.ca/files/hri25-1122.pdf">
                    <div style="width: 60px; height: 60px;">
                        <img src="img/paper_small.png" style="object-fit: contain; width: 100%; height: 100%;">
                    </div>
                </a>
                <h4><strong>Paper</strong></h4>
            </div>

            <div class="col-md-2 text-center d-flex flex-column align-items-center">
                <a href="https://drive.google.com/file/d/1Ny07f0dpc0CJyyag3KTwfpHdv_-CSz_p/view?usp=sharing">
                    <div style="width: 60px; height: 60px;">
                        <img src="img/paper_appendices.png" style="object-fit: contain; width: 100%; height: 100%;">
                    </div>
                </a>
                <h4><strong>Appendices</strong></h4>
            </div>

            <div class="col-md-2 text-center d-flex flex-column align-items-center">
                <a href="https://github.com/karthikm-0/imageinthat">
                    <div style="width: 60px; height: 60px;">
                        <img src="img/github.png" style="object-fit: contain; width: 100%; height: 100%;">
                    </div>
                </a>
                <h4><strong>Code (will be added soon!)</strong></h4>
            </div>
        </div>

        <!-- <img src="img/pipeline.gif"> -->

        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                Foundation models are rapidly improving the capability of robots in performing everyday
                                tasks autonomously such as meal preparation, yet robots will still need to be instructed
                                by humans due to model performance, the difficulty of capturing user preferences, and
                                the need for user agency. Robots can be instructed using various methods&mdash;natural
                                language conveys immediate instructions but can be abstract or ambiguous, whereas
                                end-user programming supports longer-horizon tasks but interfaces face difficulties in
                                capturing user intent. In this work, we propose using direct manipulation of images as
                                an alternative paradigm to instruct robots, and introduce a specific instantiation
                                called ImageInThat which allows users to perform direct manipulation on images in a
                                timeline-style interface to generate robot instructions. Through a user study, we
                                demonstrate the efficacy of ImageInThat to instruct robots in kitchen manipulation
                                tasks, comparing it to a text-based natural language instruction method. The results
                                show that participants were faster with ImageInThat and preferred to use it over the
                                text-based method.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Interface</h2>
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                In ImageInThat, the robot's environment is processed to create an internal
                                representation consisting of knowledge about objects and
                                fixtures. Objects are items that can be manipulated from one
                                location to another (e.g., bowls that need washing) whereas
                                fixtures are items that are immovable. Both objects and fixtures
                                can be in more than one state; for example, a cabinet is a
                                fixture that can be open or closed.

                                ImageInThat includes an editor that utilizes this internal
                                representation. The editor consists of a timeline-style interface
                                where users can manipulate images to generate robot instructions.
                                For instance, users can drag an object from one location to another
                                to generate a pick-and-place instruction. Users can also manipulate
                                fixture states, such as by clicking a cabinet to open or close it.
                                Along the way, each change appears as a step in the timeline.
                                ImageInThat also includes features to visualize changes between steps
                                inside the timeline. This includes visual filters on each step to
                                highlight changes, and a comparison view that shows the differences
                                between two steps.

                                ImageInThat also includes experimental features to allows users to
                                blend language with image to create instructions rather than relying
                                solely on image manipulation. ImageInThat can also provide some assistance
                                to the user in specifying the location of a selected object through
                                manipulation-level auto-completion. Or, the user can click a button to
                                request an image representing possible future states of the environment.
                            </p>
                        </div>
                    </div>
                </div>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <figure style="text-align:center;">
                            <img src="img/approach.png" width="100%" height="100%">
                            <figcaption style="text-align:justify; font-weight:bold; color:#333; margin-top:10px;">
                                ImageInThat’s user interface, consisting of an editor (top) and a
                                timeline (bottom). The editor (A) allows users to manipulate objects and
                                fixtures in the environment, while the timeline displays the current state of the
                                environment and the desired changes. The timeline (B) shows all instructions
                                provided to the robot. Selecting a step populates it in the editor. Changes
                                between steps are made visible by contrasting changed objects and fixtures
                                from other items (C). ImageInThat automatically captions all manipulations
                                and allows them to be edited (D). The user can instruct the robot with text to
                                generate new steps automatically (E). ImageInThat tries to predict user goals
                                such as by proposing locations where objects can be placed (F) or proposing
                                future steps (G).
                            </figcaption>
                        </figure>
                    </div>
                </div>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <video width="100%" height="auto" controls>
                            <source src="img/image_in_that_video.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                </div>
            </div>
        </section>

        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">User Evaluation</h2>
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                We evaluated ImageInThat in a user study where participants were asked to instruct a
                                robot in simulated kitchen manipulation tasks. Participants were asked to perform
                                the tasks using both ImageInThat and a text-based natural language instruction
                                baseline.
                            </p>
                        </div>
                    </div>
                </div>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <figure style="text-align:center;">
                            <img src="img/tasks.png" width="100%" height="100%">
                            <figcaption style="text-align:justify; font-weight:bold; color:#333; margin-top:10px;">
                                Tasks performed by participants in the evaluation (left to right):
                                organizing pantry, sorting fruits, cooking stir-fry, and washing dishes.
                                Depicted here is the environment state at the beginning of each task.
                            </figcaption>
                        </figure>
                    </div>
                </div>

                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                We found that participants were faster with ImageInThat than with the text-based
                                method across all tasks. However, we noticed that participants made different
                                types of errors with ImageInThat compared to the text-based method. For instance,
                                participants added extraneous steps in ImageInThat, which could be due to the
                                low-cost nature of adding steps in ImageInThat. In contrast, participants missed
                                more steps in the text-based method, which could be due to the difficulty in
                                keeping track of the steps in the text-based method.
                            </p>
                        </div>
                    </div>
                </div>
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <figure style="text-align:center;">
                            <img src="img/results.png" width="100%" height="100%">
                            <figcaption style="text-align:justify; font-weight:bold; color:#333; margin-top:10px;">
                                Left: a plot showing the number of errors for ImageInThat and the
                                text-based method. Errors are grouped into three categories: extraneous steps,
                                missing steps, and inefficient steps, and a bar displays the total count of all
                                errors. Middle: shows the task completion time per task and all tasks together.
                                All error bars are bootstrapped 95% confidence intervals. Right: shows counts
                                of responses for the NASA-TLX questionnaire.
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
        </section>


        <!-- <p style="text-align:center;">
            <img src="img/cover.png" width="60%" height="60%">
        </p> -->

        <!--    Approach    -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <!-- <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Approach</h2>
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                We aim to tackle the problem of expressive behavior generation that is both adaptive to
                                user feedback and composable so that more complex behaviors can build on simpler
                                behaviors. Formally, we define being expressive as the distance between some
                                expert expressive trajectory that could be generated by an animator (or demonstrated) \(
                                \tau_{\text{expert}} \) and a robot trajectory \( \tau \). \( \text{dist}(\tau,
                                \tau_{\text{expert}}) \) can be any desirable distance metric between the two
                                trajectories, e.g., dynamic time warping (DTW). GenEM aims to minimize this distance \(
                                d^* = \min \text{dist}(\tau,\tau_{\text{expert}}) \).
                            </p>
                            <p class="text-justify">
                                Our approach uses several LLMs in a modular fashion so that
                                each LLM agent plays a distinct role. GenEM takes user language instructions \( l_{in}
                                \in L \) as input and outputs a
                                robot policy \( \pi_\theta \), which is in the form of a parameterized code.

                                Human iterative feedback \( f_i \in L \) can be used to update the policy \( \pi_\theta
                                \).
                                The policy parameters get updated one step at a time given the feedback \( f_i \), where
                                \( i
                                \in \{1,\dots, K\} \).
                                The policy can be instantiated from some initial state \( s_0 \in S \) to produce
                                trajectories \( \tau = \{s_0, a_0, \dots, a_{N-1},s_N\} \) or instantiations of
                                expressive
                                robot behavior.
                            </p>

                        </div>
                    </div>
                </div>
            </div>
        </section> -->

        <!-- <p style="text-align:center;">
            <image src="img/approach.png" width="100%" height="100%">
        </p>

        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Studies</h2>
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                We conducted two user studies to assess whether our approach, GenEM, can be used to
                                generate expressive behaviors that are perceivable by people. We generated two versions
                                of behaviors: GenEM, and GenEM with iterative Feedback (or GenEM++). In
                                both studies, all comparisons were made against behaviors designed by a professional
                                animator and implemented by a software developer, which we term the oracle animator. In
                                the first study, our goal was to assess whether behaviors that are generated
                                using GenEM and GenEM++ would be perceived similarly to the behaviors created using the
                                oracle animator. In the second study, we attempted to generate behaviors using GenEM and
                                GenEM++ that were similar to the behaviors created using the oracle animator. Both
                                studies aim
                                to demonstrate that our approach is adaptable to human feedback.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section> -->
        <!-- End Carousel -->

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
            crossorigin="anonymous"></script>
        <script>hljs.highlightAll();</script>
</body>

</html>