<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ImageInThat: Manipulating Images to Convey User Instructions to Robots</title>

    <meta name="description" content="Generative Expressive Robot Behaviors using Large Language Models">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->

    <link href="//cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <!--script src="js/app.js"></script-->

    <style>
        .nav-pills {
            position: relative;
            display: inline;
        }

        .imtip {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>

    <link rel="stylesheet" href="./css/bulma.min.css">
    <link rel="stylesheet" href="./css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./css/bulma-slider.min.css">
    <link rel="stylesheet" href="./css/index.css">
    <link rel="stylesheet" href="./css/fontawesome.all.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/index.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5JBS73F70V"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-5JBS73F70V');
</script>

<body>
    <div class="container" id="main">
        <div class="row mt-4">
            <h2 class="col-md-12 text-center">
                <b>
                    <font size="+6">Generative Expressive Robot Behaviors <br> Using Large Language Models </br></font>
                </b>
                <small style="font-size: 20px; margin-top: 50px;">
                    HRI 2024
                </small>
                <span style="color: orange;">Best Paper (Technical Track)</span>
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <br>
                    <li><a href="https://karthikmahadevan.ca">Karthik Mahadevan</a></li>
                    <li><a href="https://www.linkedin.com/in/jonathanchien">Jonathan Chien</a></li>
                    <li><a href="https://www.linkedin.com/in/noah-brown-40157b124">Noah Brown</a></li>
                    <li><a href="https://www.linkedin.com/in/zhuo-xu-joe/">Zhuo Xu</a></li>
                    <br>
                    <li><a href="https://www.linkedin.com/in/carolinaparada/">Carolina Parada</a></li>
                    <li><a href="https://fxia22.github.io/">Fei Xia</a></li>
                    <li><a href="//andyzeng.github.io/">Andy Zeng</a></li>
                    <li><a href="https://www.linkedin.com/in/leilatakayama/">Leila Takayama</a></li>
                    <li><a href="https://dorsa.fyi/">Dorsa Sadigh</a></li>
                    <br>
                    <br>
                    <a href="http://g.co/robotics">
                        <img src="img/deepmind.png" style="width: 10%;">
                        Google Deepmind
                    </a>
                </ul>
            </div>
        </div>

        <div class="row justify-content-md-center" style="margin-top: 50px; margin-bottom: 50px;">
            <div class="col-md-2 text-center d-flex flex-column align-items-center">
                <a href="https://arxiv.org/abs/2401.14673">
                    <div style="width: 60px; height: 60px;">
                        <img src="img/paper_small.png" style="object-fit: contain; width: 100%; height: 100%;">
                    </div>
                </a>
                <h4><strong>Paper</strong></h4>
            </div>
        
            <div class="col-md-2 text-center d-flex flex-column align-items-center">
                <a href="https://drive.google.com/file/d/1Ny07f0dpc0CJyyag3KTwfpHdv_-CSz_p/view?usp=sharing">
                    <div style="width: 60px; height: 60px;">
                        <img src="img/paper_appendices.png" style="object-fit: contain; width: 100%; height: 100%;">
                    </div>
                </a>
                <h4><strong>Appendices</strong></h4>
            </div>
        </div>

        <!-- <img src="img/pipeline.gif"> -->

        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                People employ expressive behaviors to effectively communicate and coordinate their
                                actions with
                                others, such as nodding to acknowledge a person glancing at them or saying "excuse me"
                                to pass
                                people in a busy corridor. We would like robots to also demonstrate expressive behaviors
                                in
                                human-robot interaction. Prior work proposes rule-based methods that struggle to scale
                                to new
                                communication modalities or social situations, while data-driven methods require
                                specialized
                                datasets for each social situation the robot is used in. We propose to leverage the rich
                                social
                                context available from large language models (LLMs) and their ability to generate motion
                                based on
                                instructions or user preferences, to generate expressive robot motion that is adaptable
                                and
                                composable, building upon each other. Our approach utilizes few-shot chain-of-thought
                                prompting to
                                translate human language instructions into parametrized control code using the robot's
                                available and
                                learned skills. Through user studies and simulation experiments, we demonstrate that our
                                approach
                                produces behaviors that users found to be competent and easy to understand.
                                Supplementary material
                                can be found at https://generative-expressive-motion.github.io/.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <p style="text-align:center;">
            <img src="img/cover.png" width="60%" height="60%">
        </p>

        <!--    Approach    -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Approach</h2>
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                We aim to tackle the problem of expressive behavior generation that is both adaptive to
                                user feedback and composable so that more complex behaviors can build on simpler
                                behaviors. Formally, we define being expressive as the distance between some
                                expert expressive trajectory that could be generated by an animator (or demonstrated) \(
                                \tau_{\text{expert}} \) and a robot trajectory \( \tau \). \( \text{dist}(\tau,
                                \tau_{\text{expert}}) \) can be any desirable distance metric between the two
                                trajectories, e.g., dynamic time warping (DTW). GenEM aims to minimize this distance \(
                                d^* = \min \text{dist}(\tau,\tau_{\text{expert}}) \).
                            </p>
                            <p class="text-justify">
                                Our approach uses several LLMs in a modular fashion so that
                                each LLM agent plays a distinct role. GenEM takes user language instructions \( l_{in}
                                \in L \) as input and outputs a
                                robot policy \( \pi_\theta \), which is in the form of a parameterized code.

                                Human iterative feedback \( f_i \in L \) can be used to update the policy \( \pi_\theta
                                \).
                                The policy parameters get updated one step at a time given the feedback \( f_i \), where
                                \( i
                                \in \{1,\dots, K\} \).
                                The policy can be instantiated from some initial state \( s_0 \in S \) to produce
                                trajectories \( \tau = \{s_0, a_0, \dots, a_{N-1},s_N\} \) or instantiations of
                                expressive
                                robot behavior.
                            </p>

                        </div>
                    </div>
                </div>
            </div>
        </section>

        <p style="text-align:center;">
            <image src="img/approach.png" width="100%" height="100%">
        </p>

        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Studies</h2>
                        <div class="content has-text-justified">
                            <p class="text-justify">
                                We conducted two user studies to assess whether our approach, GenEM, can be used to
                                generate expressive behaviors that are perceivable by people. We generated two versions
                                of behaviors: GenEM, and GenEM with iterative Feedback (or GenEM++). In
                                both studies, all comparisons were made against behaviors designed by a professional
                                animator and implemented by a software developer, which we term the oracle animator. In
                                the first study, our goal was to assess whether behaviors that are generated
                                using GenEM and GenEM++ would be perceived similarly to the behaviors created using the
                                oracle animator. In the second study, we attempted to generate behaviors using GenEM and
                                GenEM++ that were similar to the behaviors created using the oracle animator. Both
                                studies aim
                                to demonstrate that our approach is adaptable to human feedback.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- End Carousel -->

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
            crossorigin="anonymous"></script>
        <script>hljs.highlightAll();</script>
</body>

</html>